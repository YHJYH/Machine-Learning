multiple features<br>
gradient descent on multiple variables<br>
<br>
<b>features scaling</b>: make sure features are on a smiliar scale<br>
method: x_n = (x_n - miu_n)/s_n <b>mean normalization</b>: make features have approximately zero mean<br>
miu_n = average , s_n = standardized error<br>
<br>
test the gradient descent: <b>min<i>J</i> converges</b><br>
plot cost function <i>J</i> against number of iterations (a converging curve)<br>
if learning rate is <b>too small</b>: slow convergence<br>
if learning rate is <b>too large</b>: no convergence<br>
<br>
polynomial regression<br>
can transfer to linear regression<br>
e.g. x_2 = x^2, x_3 = x^3<br>
need feature scaling<br>
<br>
<b>normal equation</b>: solve for theta in J(theta) analytically.<br>


